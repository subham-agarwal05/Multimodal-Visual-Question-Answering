{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd087033",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-12T18:13:45.914992Z",
     "iopub.status.busy": "2025-05-12T18:13:45.914672Z",
     "iopub.status.idle": "2025-05-12T18:13:45.924880Z",
     "shell.execute_reply": "2025-05-12T18:13:45.922496Z",
     "shell.execute_reply.started": "2025-05-12T18:13:45.914959Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import gzip\n",
    "import shutil\n",
    "import os\n",
    "import json\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca5ef10",
   "metadata": {},
   "source": [
    "## Data extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "86033502",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-12T18:30:10.967227Z",
     "iopub.status.busy": "2025-05-12T18:30:10.966743Z",
     "iopub.status.idle": "2025-05-12T18:30:10.976795Z",
     "shell.execute_reply": "2025-05-12T18:30:10.975885Z",
     "shell.execute_reply.started": "2025-05-12T18:30:10.967194Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extraction process finished.\n"
     ]
    }
   ],
   "source": [
    "source_dir = '/kaggle/input/abo-listings/'\n",
    "destination_dir = '/kaggle/working/abo-listings/listings/extracted_metadata'\n",
    "\n",
    "os.makedirs(destination_dir, exist_ok=True)\n",
    "\n",
    "compressed_files = [file for file in os.listdir(source_dir) if file.endswith('.json.gz')]\n",
    "\n",
    "for compressed_file in compressed_files:\n",
    "    src_path = os.path.join(source_dir, compressed_file)\n",
    "    dest_filename = compressed_file.replace('.gz', '')  # strips only .gz\n",
    "    dest_path = os.path.join(destination_dir, dest_filename)\n",
    "\n",
    "    with gzip.open(src_path, 'rb') as source_file, open(dest_path, 'wb') as target_file:\n",
    "        shutil.copyfileobj(source_file, target_file)\n",
    "\n",
    "    print(f\"Decompressed: {compressed_file} → {dest_filename}\")\n",
    "\n",
    "print(\"Extraction process finished.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0eef8ed",
   "metadata": {},
   "source": [
    "## Filtering data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e0748c1f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-12T18:31:37.206453Z",
     "iopub.status.busy": "2025-05-12T18:31:37.206148Z",
     "iopub.status.idle": "2025-05-12T18:32:09.010001Z",
     "shell.execute_reply": "2025-05-12T18:32:09.009038Z",
     "shell.execute_reply.started": "2025-05-12T18:31:37.206429Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: /kaggle/input/abo-listings/listings_3.json → /kaggle/working/abo-listings/listings/filtered_metadata/listings_3.csv\n",
      " → Valid entries found: 4193\n",
      " → Output written: /kaggle/working/abo-listings/listings/filtered_metadata/listings_3.csv (4193 rows)\n",
      "\n",
      "Processing file: /kaggle/input/abo-listings/listings_d.json → /kaggle/working/abo-listings/listings/filtered_metadata/listings_d.csv\n",
      " → Valid entries found: 4178\n",
      " → Output written: /kaggle/working/abo-listings/listings/filtered_metadata/listings_d.csv (4178 rows)\n",
      "\n",
      "Processing file: /kaggle/input/abo-listings/listings_b.json → /kaggle/working/abo-listings/listings/filtered_metadata/listings_b.csv\n",
      " → Valid entries found: 4279\n",
      " → Output written: /kaggle/working/abo-listings/listings/filtered_metadata/listings_b.csv (4279 rows)\n",
      "\n",
      "Processing file: /kaggle/input/abo-listings/listings_a.json → /kaggle/working/abo-listings/listings/filtered_metadata/listings_a.csv\n",
      " → Valid entries found: 4232\n",
      " → Output written: /kaggle/working/abo-listings/listings/filtered_metadata/listings_a.csv (4232 rows)\n",
      "\n",
      "Processing file: /kaggle/input/abo-listings/listings_4.json → /kaggle/working/abo-listings/listings/filtered_metadata/listings_4.csv\n",
      " → Valid entries found: 4134\n",
      " → Output written: /kaggle/working/abo-listings/listings/filtered_metadata/listings_4.csv (4134 rows)\n",
      "\n",
      "Processing file: /kaggle/input/abo-listings/listings_8.json → /kaggle/working/abo-listings/listings/filtered_metadata/listings_8.csv\n",
      " → Valid entries found: 4207\n",
      " → Output written: /kaggle/working/abo-listings/listings/filtered_metadata/listings_8.csv (4207 rows)\n",
      "\n",
      "Processing file: /kaggle/input/abo-listings/listings_9.json → /kaggle/working/abo-listings/listings/filtered_metadata/listings_9.csv\n",
      " → Valid entries found: 4152\n",
      " → Output written: /kaggle/working/abo-listings/listings/filtered_metadata/listings_9.csv (4152 rows)\n",
      "\n",
      "Processing file: /kaggle/input/abo-listings/listings_f.json → /kaggle/working/abo-listings/listings/filtered_metadata/listings_f.csv\n",
      " → Valid entries found: 4255\n",
      " → Output written: /kaggle/working/abo-listings/listings/filtered_metadata/listings_f.csv (4255 rows)\n",
      "\n",
      "Processing file: /kaggle/input/abo-listings/listings_0.json → /kaggle/working/abo-listings/listings/filtered_metadata/listings_0.csv\n",
      " → Valid entries found: 4258\n",
      " → Output written: /kaggle/working/abo-listings/listings/filtered_metadata/listings_0.csv (4258 rows)\n",
      "\n",
      "Processing file: /kaggle/input/abo-listings/listings_6.json → /kaggle/working/abo-listings/listings/filtered_metadata/listings_6.csv\n",
      " → Valid entries found: 4209\n",
      " → Output written: /kaggle/working/abo-listings/listings/filtered_metadata/listings_6.csv (4209 rows)\n",
      "\n",
      "Processing file: /kaggle/input/abo-listings/listings_c.json → /kaggle/working/abo-listings/listings/filtered_metadata/listings_c.csv\n",
      " → Valid entries found: 4274\n",
      " → Output written: /kaggle/working/abo-listings/listings/filtered_metadata/listings_c.csv (4274 rows)\n",
      "\n",
      "Processing file: /kaggle/input/abo-listings/listings_2.json → /kaggle/working/abo-listings/listings/filtered_metadata/listings_2.csv\n",
      " → Valid entries found: 4223\n",
      " → Output written: /kaggle/working/abo-listings/listings/filtered_metadata/listings_2.csv (4223 rows)\n",
      "\n",
      "Processing file: /kaggle/input/abo-listings/listings_5.json → /kaggle/working/abo-listings/listings/filtered_metadata/listings_5.csv\n",
      " → Valid entries found: 4271\n",
      " → Output written: /kaggle/working/abo-listings/listings/filtered_metadata/listings_5.csv (4271 rows)\n",
      "\n",
      "Processing file: /kaggle/input/abo-listings/listings_1.json → /kaggle/working/abo-listings/listings/filtered_metadata/listings_1.csv\n",
      " → Valid entries found: 4208\n",
      " → Output written: /kaggle/working/abo-listings/listings/filtered_metadata/listings_1.csv (4208 rows)\n",
      "\n",
      "Processing file: /kaggle/input/abo-listings/listings_e.json → /kaggle/working/abo-listings/listings/filtered_metadata/listings_e.csv\n",
      " → Valid entries found: 4127\n",
      " → Output written: /kaggle/working/abo-listings/listings/filtered_metadata/listings_e.csv (4127 rows)\n",
      "\n",
      "Processing file: /kaggle/input/abo-listings/listings_7.json → /kaggle/working/abo-listings/listings/filtered_metadata/listings_7.csv\n",
      " → Valid entries found: 4166\n",
      " → Output written: /kaggle/working/abo-listings/listings/filtered_metadata/listings_7.csv (4166 rows)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define directories\n",
    "source_dir = '/kaggle/input/abo-listings'\n",
    "target_dir = '/kaggle/working/abo-listings/listings/filtered_metadata'\n",
    "\n",
    "# Ensure output directory exists\n",
    "os.makedirs(target_dir, exist_ok=True)\n",
    "\n",
    "# Define output CSV header\n",
    "csv_columns = [\n",
    "    'main_image_id', 'overall_description', 'colour_description',\n",
    "    'other_description', 'material_description'\n",
    "]\n",
    "\n",
    "# Utility: Extract 'value' fields filtered by language\n",
    "def extract_values_by_language(entries):\n",
    "    return [\n",
    "        entry['value'] for entry in entries\n",
    "        if 'value' in entry and (\n",
    "            'language_tag' not in entry or entry['language_tag'] in {'en_US', 'en_IN'}\n",
    "        )\n",
    "    ]\n",
    "\n",
    "# Utility: Extract 'standardized_values' from color fields filtered by language\n",
    "def extract_standardized_colors(entries):\n",
    "    standardized = []\n",
    "    for entry in entries:\n",
    "        if 'language_tag' not in entry or entry['language_tag'] in {'en_US', 'en_IN'}:\n",
    "            standardized.extend(entry.get('standardized_values', []))\n",
    "    return standardized\n",
    "\n",
    "# Loop through each JSON file\n",
    "for file_name in os.listdir(source_dir):\n",
    "    if not file_name.endswith('.json'):\n",
    "        continue\n",
    "\n",
    "    json_path = os.path.join(source_dir, file_name)\n",
    "    csv_path = os.path.join(target_dir, file_name.replace('.json', '.csv'))\n",
    "\n",
    "    print(f\"Processing file: {json_path} → {csv_path}\")\n",
    "\n",
    "    # Load JSON lines\n",
    "    records = []\n",
    "    with open(json_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            line = line.strip()\n",
    "            if line:\n",
    "                try:\n",
    "                    records.append(json.loads(line))\n",
    "                except json.JSONDecodeError as error:\n",
    "                    print(f\"Error parsing line in {file_name}: {error}\")\n",
    "\n",
    "    # Filter out incomplete or irrelevant records\n",
    "    required_fields = {\n",
    "        'brand', 'bullet_point', 'color', 'model_name',\n",
    "        'item_name', 'product_type', 'main_image_id',\n",
    "        'item_keywords', 'country'\n",
    "    }\n",
    "\n",
    "    valid_records = [\n",
    "        rec for rec in records\n",
    "        if required_fields.issubset(rec.keys()) and rec.get('country') in {'IN', 'US'}\n",
    "    ]\n",
    "\n",
    "    print(f\" → Valid entries found: {len(valid_records)}\")\n",
    "\n",
    "    # Write filtered records to CSV\n",
    "    with open(csv_path, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow(csv_columns)\n",
    "\n",
    "        for rec in valid_records:\n",
    "            overall_desc = extract_values_by_language(rec.get('bullet_point', []))\n",
    "\n",
    "            color_desc = []\n",
    "            color_desc += extract_standardized_colors(rec.get('color', []))\n",
    "            color_desc += extract_values_by_language(rec.get('color', []))\n",
    "\n",
    "            other_desc = []\n",
    "            for key in ['product_type', 'item_keywords']:\n",
    "                other_desc += extract_values_by_language(rec.get(key, []))\n",
    "\n",
    "            material_desc = extract_values_by_language(rec.get('material', [])) if 'material' in rec else []\n",
    "\n",
    "            row = [\n",
    "                rec.get('main_image_id', ''),\n",
    "                '; '.join(overall_desc),\n",
    "                '; '.join(color_desc),\n",
    "                '; '.join(other_desc),\n",
    "                '; '.join(material_desc)\n",
    "            ]\n",
    "            writer.writerow(row)\n",
    "\n",
    "    print(f\" → Output written: {csv_path} ({len(valid_records)} rows)\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fcd20b4d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-12T18:15:15.036680Z",
     "iopub.status.busy": "2025-05-12T18:15:15.036372Z",
     "iopub.status.idle": "2025-05-12T18:15:24.937013Z",
     "shell.execute_reply": "2025-05-12T18:15:24.935875Z",
     "shell.execute_reply.started": "2025-05-12T18:15:15.036651Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /usr/local/lib/python3.11/dist-packages (24.1.2)\n",
      "Collecting pip\n",
      "  Downloading pip-25.1.1-py3-none-any.whl.metadata (3.6 kB)\n",
      "Downloading pip-25.1.1-py3-none-any.whl (1.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m35.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pip\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 24.1.2\n",
      "    Uninstalling pip-24.1.2:\n",
      "      Successfully uninstalled pip-24.1.2\n",
      "Successfully installed pip-25.1.1\n",
      "Requirement already satisfied: google-genai in /usr/local/lib/python3.11/dist-packages (0.8.0)\n",
      "Requirement already satisfied: google-auth<3.0.0dev,>=2.14.1 in /usr/local/lib/python3.11/dist-packages (from google-genai) (2.27.0)\n",
      "Requirement already satisfied: pydantic<3.0.0dev,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-genai) (2.11.3)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.28.1 in /usr/local/lib/python3.11/dist-packages (from google-genai) (2.32.3)\n",
      "Requirement already satisfied: websockets<15.0dev,>=13.0 in /usr/local/lib/python3.11/dist-packages (from google-genai) (14.2)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth<3.0.0dev,>=2.14.1->google-genai) (5.5.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth<3.0.0dev,>=2.14.1->google-genai) (0.4.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth<3.0.0dev,>=2.14.1->google-genai) (4.9)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0dev,>=2.0.0->google-genai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0dev,>=2.0.0->google-genai) (2.33.1)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0dev,>=2.0.0->google-genai) (4.13.1)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0dev,>=2.0.0->google-genai) (0.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0dev,>=2.28.1->google-genai) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0dev,>=2.28.1->google-genai) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0dev,>=2.28.1->google-genai) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0dev,>=2.28.1->google-genai) (2025.1.31)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.11/dist-packages (from rsa<5,>=3.1.4->google-auth<3.0.0dev,>=2.14.1->google-genai) (0.6.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pip\n",
    "!pip install google-genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "83fa80a7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-12T18:56:31.048543Z",
     "iopub.status.busy": "2025-05-12T18:56:31.048172Z",
     "iopub.status.idle": "2025-05-12T18:56:31.055469Z",
     "shell.execute_reply": "2025-05-12T18:56:31.054232Z",
     "shell.execute_reply.started": "2025-05-12T18:56:31.048517Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Client', '__all__', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__path__', '__spec__', '__version__', '_api_client', '_api_module', '_common', '_extra_utils', '_replay_api_client', '_transformers', 'batches', 'caches', 'chats', 'client', 'errors', 'files', 'live', 'models', 'pagers', 'tunings', 'types', 'version']\n",
      "['__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_get_api_client', 'aio', 'batches', 'caches', 'chats', 'files', 'models', 'tunings', 'vertexai']\n",
      "['AdapterSize', 'Any', 'AutomaticFunctionCallingConfig', 'AutomaticFunctionCallingConfigDict', 'AutomaticFunctionCallingConfigOrDict', 'BatchJob', 'BatchJobDestination', 'BatchJobDestinationDict', 'BatchJobDestinationOrDict', 'BatchJobDict', 'BatchJobOrDict', 'BatchJobSource', 'BatchJobSourceDict', 'BatchJobSourceOrDict', 'Blob', 'BlobDict', 'BlobOrDict', 'BlockedReason', 'CachedContent', 'CachedContentDict', 'CachedContentOrDict', 'CachedContentUsageMetadata', 'CachedContentUsageMetadataDict', 'CachedContentUsageMetadataOrDict', 'Callable', 'CancelBatchJobConfig', 'CancelBatchJobConfigDict', 'CancelBatchJobConfigOrDict', 'Candidate', 'CandidateDict', 'CandidateOrDict', 'Citation', 'CitationDict', 'CitationMetadata', 'CitationMetadataDict', 'CitationMetadataOrDict', 'CitationOrDict', 'CodeExecutionResult', 'CodeExecutionResultDict', 'CodeExecutionResultOrDict', 'ComputeTokensConfig', 'ComputeTokensConfigDict', 'ComputeTokensConfigOrDict', 'ComputeTokensResponse', 'ComputeTokensResponseDict', 'ComputeTokensResponseOrDict', 'Content', 'ContentDict', 'ContentEmbedding', 'ContentEmbeddingDict', 'ContentEmbeddingOrDict', 'ContentEmbeddingStatistics', 'ContentEmbeddingStatisticsDict', 'ContentEmbeddingStatisticsOrDict', 'ContentListUnion', 'ContentListUnionDict', 'ContentOrDict', 'ContentUnion', 'ContentUnionDict', 'ControlReferenceConfig', 'ControlReferenceConfigDict', 'ControlReferenceConfigOrDict', 'ControlReferenceImage', 'ControlReferenceImageDict', 'ControlReferenceImageOrDict', 'ControlReferenceType', 'CountTokensConfig', 'CountTokensConfigDict', 'CountTokensConfigOrDict', 'CountTokensResponse', 'CountTokensResponseDict', 'CountTokensResponseOrDict', 'CreateBatchJobConfig', 'CreateBatchJobConfigDict', 'CreateBatchJobConfigOrDict', 'CreateCachedContentConfig', 'CreateCachedContentConfigDict', 'CreateCachedContentConfigOrDict', 'CreateFileConfig', 'CreateFileConfigDict', 'CreateFileConfigOrDict', 'CreateFileResponse', 'CreateFileResponseDict', 'CreateFileResponseOrDict', 'CreateTuningJobConfig', 'CreateTuningJobConfigDict', 'CreateTuningJobConfigOrDict', 'DatasetDistribution', 'DatasetDistributionDict', 'DatasetDistributionDistributionBucket', 'DatasetDistributionDistributionBucketDict', 'DatasetDistributionDistributionBucketOrDict', 'DatasetDistributionOrDict', 'DatasetStats', 'DatasetStatsDict', 'DatasetStatsOrDict', 'DeleteBatchJobConfig', 'DeleteBatchJobConfigDict', 'DeleteBatchJobConfigOrDict', 'DeleteCachedContentConfig', 'DeleteCachedContentConfigDict', 'DeleteCachedContentConfigOrDict', 'DeleteCachedContentResponse', 'DeleteCachedContentResponseDict', 'DeleteCachedContentResponseOrDict', 'DeleteFileConfig', 'DeleteFileConfigDict', 'DeleteFileConfigOrDict', 'DeleteFileResponse', 'DeleteFileResponseDict', 'DeleteFileResponseOrDict', 'DeleteModelConfig', 'DeleteModelConfigDict', 'DeleteModelConfigOrDict', 'DeleteModelResponse', 'DeleteModelResponseDict', 'DeleteModelResponseOrDict', 'DeleteResourceJob', 'DeleteResourceJobDict', 'DeleteResourceJobOrDict', 'DeploymentResourcesType', 'DistillationDataStats', 'DistillationDataStatsDict', 'DistillationDataStatsOrDict', 'DistillationHyperParameters', 'DistillationHyperParametersDict', 'DistillationHyperParametersOrDict', 'DistillationSpec', 'DistillationSpecDict', 'DistillationSpecOrDict', 'DownloadFileConfig', 'DownloadFileConfigDict', 'DownloadFileConfigOrDict', 'DynamicRetrievalConfig', 'DynamicRetrievalConfigDict', 'DynamicRetrievalConfigMode', 'DynamicRetrievalConfigOrDict', 'EditImageConfig', 'EditImageConfigDict', 'EditImageConfigOrDict', 'EditImageResponse', 'EditImageResponseDict', 'EditImageResponseOrDict', 'EditMode', 'EmbedContentConfig', 'EmbedContentConfigDict', 'EmbedContentConfigOrDict', 'EmbedContentMetadata', 'EmbedContentMetadataDict', 'EmbedContentMetadataOrDict', 'EmbedContentResponse', 'EmbedContentResponseDict', 'EmbedContentResponseOrDict', 'EncryptionSpec', 'EncryptionSpecDict', 'EncryptionSpecOrDict', 'Endpoint', 'EndpointDict', 'EndpointOrDict', 'Enum', 'EnumMeta', 'ExecutableCode', 'ExecutableCodeDict', 'ExecutableCodeOrDict', 'FetchPredictOperationConfig', 'FetchPredictOperationConfigDict', 'FetchPredictOperationConfigOrDict', 'Field', 'File', 'FileData', 'FileDataDict', 'FileDataOrDict', 'FileDict', 'FileOrDict', 'FileSource', 'FileState', 'FileStatus', 'FileStatusDict', 'FileStatusOrDict', 'FinishReason', 'FunctionCall', 'FunctionCallDict', 'FunctionCallOrDict', 'FunctionCallingConfig', 'FunctionCallingConfigDict', 'FunctionCallingConfigMode', 'FunctionCallingConfigOrDict', 'FunctionDeclaration', 'FunctionDeclarationDict', 'FunctionDeclarationOrDict', 'FunctionResponse', 'FunctionResponseDict', 'FunctionResponseOrDict', 'GenerateContentConfig', 'GenerateContentConfigDict', 'GenerateContentConfigOrDict', 'GenerateContentResponse', 'GenerateContentResponseDict', 'GenerateContentResponseOrDict', 'GenerateContentResponsePromptFeedback', 'GenerateContentResponsePromptFeedbackDict', 'GenerateContentResponsePromptFeedbackOrDict', 'GenerateContentResponseUsageMetadata', 'GenerateContentResponseUsageMetadataDict', 'GenerateContentResponseUsageMetadataOrDict', 'GenerateImagesConfig', 'GenerateImagesConfigDict', 'GenerateImagesConfigOrDict', 'GenerateImagesResponse', 'GenerateImagesResponseDict', 'GenerateImagesResponseOrDict', 'GeneratedImage', 'GeneratedImageDict', 'GeneratedImageOrDict', 'GenerationConfig', 'GenerationConfigDict', 'GenerationConfigOrDict', 'GenerationConfigRoutingConfig', 'GenerationConfigRoutingConfigAutoRoutingMode', 'GenerationConfigRoutingConfigAutoRoutingModeDict', 'GenerationConfigRoutingConfigAutoRoutingModeOrDict', 'GenerationConfigRoutingConfigDict', 'GenerationConfigRoutingConfigManualRoutingMode', 'GenerationConfigRoutingConfigManualRoutingModeDict', 'GenerationConfigRoutingConfigManualRoutingModeOrDict', 'GenerationConfigRoutingConfigOrDict', 'GenericAlias', 'GetBatchJobConfig', 'GetBatchJobConfigDict', 'GetBatchJobConfigOrDict', 'GetCachedContentConfig', 'GetCachedContentConfigDict', 'GetCachedContentConfigOrDict', 'GetFileConfig', 'GetFileConfigDict', 'GetFileConfigOrDict', 'GetModelConfig', 'GetModelConfigDict', 'GetModelConfigOrDict', 'GetOperationConfig', 'GetOperationConfigDict', 'GetOperationConfigOrDict', 'GetTuningJobConfig', 'GetTuningJobConfigDict', 'GetTuningJobConfigOrDict', 'GoogleRpcStatus', 'GoogleRpcStatusDict', 'GoogleRpcStatusOrDict', 'GoogleSearch', 'GoogleSearchDict', 'GoogleSearchOrDict', 'GoogleSearchRetrieval', 'GoogleSearchRetrievalDict', 'GoogleSearchRetrievalOrDict', 'GoogleTypeDate', 'GoogleTypeDateDict', 'GoogleTypeDateOrDict', 'GroundingChunk', 'GroundingChunkDict', 'GroundingChunkOrDict', 'GroundingChunkRetrievedContext', 'GroundingChunkRetrievedContextDict', 'GroundingChunkRetrievedContextOrDict', 'GroundingChunkWeb', 'GroundingChunkWebDict', 'GroundingChunkWebOrDict', 'GroundingMetadata', 'GroundingMetadataDict', 'GroundingMetadataOrDict', 'GroundingSupport', 'GroundingSupportDict', 'GroundingSupportOrDict', 'HarmBlockMethod', 'HarmBlockThreshold', 'HarmCategory', 'HarmProbability', 'HarmSeverity', 'HttpOptions', 'HttpOptionsDict', 'HttpOptionsOrDict', 'Image', 'ImageDict', 'ImageOrDict', 'ImagePromptLanguage', 'JOB_STATES_ENDED', 'JOB_STATES_ENDED_MLDEV', 'JOB_STATES_ENDED_VERTEX', 'JOB_STATES_SUCCEEDED', 'JOB_STATES_SUCCEEDED_MLDEV', 'JOB_STATES_SUCCEEDED_VERTEX', 'JobError', 'JobErrorDict', 'JobErrorOrDict', 'JobState', 'Language', 'ListBatchJobsConfig', 'ListBatchJobsConfigDict', 'ListBatchJobsConfigOrDict', 'ListBatchJobsResponse', 'ListBatchJobsResponseDict', 'ListBatchJobsResponseOrDict', 'ListCachedContentsConfig', 'ListCachedContentsConfigDict', 'ListCachedContentsConfigOrDict', 'ListCachedContentsResponse', 'ListCachedContentsResponseDict', 'ListCachedContentsResponseOrDict', 'ListFilesConfig', 'ListFilesConfigDict', 'ListFilesConfigOrDict', 'ListFilesResponse', 'ListFilesResponseDict', 'ListFilesResponseOrDict', 'ListModelsConfig', 'ListModelsConfigDict', 'ListModelsConfigOrDict', 'ListModelsResponse', 'ListModelsResponseDict', 'ListModelsResponseOrDict', 'ListTuningJobsConfig', 'ListTuningJobsConfigDict', 'ListTuningJobsConfigOrDict', 'ListTuningJobsResponse', 'ListTuningJobsResponseDict', 'ListTuningJobsResponseOrDict', 'Literal', 'LiveClientContent', 'LiveClientContentDict', 'LiveClientContentOrDict', 'LiveClientMessage', 'LiveClientMessageDict', 'LiveClientMessageOrDict', 'LiveClientRealtimeInput', 'LiveClientRealtimeInputDict', 'LiveClientRealtimeInputOrDict', 'LiveClientSetup', 'LiveClientSetupDict', 'LiveClientSetupOrDict', 'LiveClientToolResponse', 'LiveClientToolResponseDict', 'LiveClientToolResponseOrDict', 'LiveConnectConfig', 'LiveConnectConfigDict', 'LiveConnectConfigOrDict', 'LiveServerContent', 'LiveServerContentDict', 'LiveServerContentOrDict', 'LiveServerMessage', 'LiveServerMessageDict', 'LiveServerMessageOrDict', 'LiveServerSetupComplete', 'LiveServerSetupCompleteDict', 'LiveServerSetupCompleteOrDict', 'LiveServerToolCall', 'LiveServerToolCallCancellation', 'LiveServerToolCallCancellationDict', 'LiveServerToolCallCancellationOrDict', 'LiveServerToolCallDict', 'LiveServerToolCallOrDict', 'LogprobsResult', 'LogprobsResultCandidate', 'LogprobsResultCandidateDict', 'LogprobsResultCandidateOrDict', 'LogprobsResultDict', 'LogprobsResultOrDict', 'LogprobsResultTopCandidates', 'LogprobsResultTopCandidatesDict', 'LogprobsResultTopCandidatesOrDict', 'MaskReferenceConfig', 'MaskReferenceConfigDict', 'MaskReferenceConfigOrDict', 'MaskReferenceImage', 'MaskReferenceImageDict', 'MaskReferenceImageOrDict', 'MaskReferenceMode', 'MediaResolution', 'Modality', 'Mode', 'Model', 'ModelDict', 'ModelOrDict', 'Operation', 'OperationDict', 'OperationOrDict', 'Optional', 'Outcome', 'PIL', 'PIL_Image', 'Part', 'PartDict', 'PartOrDict', 'PartUnion', 'PartUnionDict', 'PartnerModelTuningSpec', 'PartnerModelTuningSpecDict', 'PartnerModelTuningSpecOrDict', 'PersonGeneration', 'PrebuiltVoiceConfig', 'PrebuiltVoiceConfigDict', 'PrebuiltVoiceConfigOrDict', 'RawReferenceImage', 'RawReferenceImageDict', 'RawReferenceImageOrDict', 'ReplayFile', 'ReplayFileDict', 'ReplayFileOrDict', 'ReplayInteraction', 'ReplayInteractionDict', 'ReplayInteractionOrDict', 'ReplayRequest', 'ReplayRequestDict', 'ReplayRequestOrDict', 'ReplayResponse', 'ReplayResponseDict', 'ReplayResponseOrDict', 'Retrieval', 'RetrievalDict', 'RetrievalMetadata', 'RetrievalMetadataDict', 'RetrievalMetadataOrDict', 'RetrievalOrDict', 'SafetyFilterLevel', 'SafetyRating', 'SafetyRatingDict', 'SafetyRatingOrDict', 'SafetySetting', 'SafetySettingDict', 'SafetySettingOrDict', 'Schema', 'SchemaDict', 'SchemaOrDict', 'SchemaUnion', 'SchemaUnionDict', 'SearchEntryPoint', 'SearchEntryPointDict', 'SearchEntryPointOrDict', 'Segment', 'SegmentDict', 'SegmentOrDict', 'SpeechConfig', 'SpeechConfigDict', 'SpeechConfigOrDict', 'SpeechConfigUnion', 'SpeechConfigUnionDict', 'State', 'StyleReferenceConfig', 'StyleReferenceConfigDict', 'StyleReferenceConfigOrDict', 'StyleReferenceImage', 'StyleReferenceImageDict', 'StyleReferenceImageOrDict', 'SubjectReferenceConfig', 'SubjectReferenceConfigDict', 'SubjectReferenceConfigOrDict', 'SubjectReferenceImage', 'SubjectReferenceImageDict', 'SubjectReferenceImageOrDict', 'SubjectReferenceType', 'SupervisedHyperParameters', 'SupervisedHyperParametersDict', 'SupervisedHyperParametersOrDict', 'SupervisedTuningDataStats', 'SupervisedTuningDataStatsDict', 'SupervisedTuningDataStatsOrDict', 'SupervisedTuningDatasetDistribution', 'SupervisedTuningDatasetDistributionDatasetBucket', 'SupervisedTuningDatasetDistributionDatasetBucketDict', 'SupervisedTuningDatasetDistributionDatasetBucketOrDict', 'SupervisedTuningDatasetDistributionDict', 'SupervisedTuningDatasetDistributionOrDict', 'SupervisedTuningSpec', 'SupervisedTuningSpecDict', 'SupervisedTuningSpecOrDict', 'TestTableFile', 'TestTableFileDict', 'TestTableFileOrDict', 'TestTableItem', 'TestTableItemDict', 'TestTableItemOrDict', 'ThinkingConfig', 'ThinkingConfigDict', 'ThinkingConfigOrDict', 'TokensInfo', 'TokensInfoDict', 'TokensInfoOrDict', 'Tool', 'ToolCodeExecution', 'ToolCodeExecutionDict', 'ToolCodeExecutionOrDict', 'ToolConfig', 'ToolConfigDict', 'ToolConfigOrDict', 'ToolDict', 'ToolListUnion', 'ToolListUnionDict', 'ToolOrDict', 'TunedModel', 'TunedModelDict', 'TunedModelInfo', 'TunedModelInfoDict', 'TunedModelInfoOrDict', 'TunedModelOrDict', 'TuningDataStats', 'TuningDataStatsDict', 'TuningDataStatsOrDict', 'TuningDataset', 'TuningDatasetDict', 'TuningDatasetOrDict', 'TuningExample', 'TuningExampleDict', 'TuningExampleOrDict', 'TuningJob', 'TuningJobDict', 'TuningJobOrDict', 'TuningJobOrOperation', 'TuningJobOrOperationDict', 'TuningJobOrOperationOrDict', 'TuningValidationDataset', 'TuningValidationDatasetDict', 'TuningValidationDatasetOrDict', 'Type', 'TypedDict', 'Union', 'UpdateCachedContentConfig', 'UpdateCachedContentConfigDict', 'UpdateCachedContentConfigOrDict', 'UpdateModelConfig', 'UpdateModelConfigDict', 'UpdateModelConfigOrDict', 'UploadFileConfig', 'UploadFileConfigDict', 'UploadFileConfigOrDict', 'UpscaleImageConfig', 'UpscaleImageConfigDict', 'UpscaleImageConfigOrDict', 'UpscaleImageParameters', 'UpscaleImageParametersDict', 'UpscaleImageParametersOrDict', 'UpscaleImageResponse', 'UpscaleImageResponseDict', 'UpscaleImageResponseOrDict', 'VertexAISearch', 'VertexAISearchDict', 'VertexAISearchOrDict', 'VertexRagStore', 'VertexRagStoreDict', 'VertexRagStoreOrDict', 'VertexRagStoreRagResource', 'VertexRagStoreRagResourceDict', 'VertexRagStoreRagResourceOrDict', 'VideoMetadata', 'VideoMetadataDict', 'VideoMetadataOrDict', 'VoiceConfig', 'VoiceConfigDict', 'VoiceConfigOrDict', '_CancelBatchJobParameters', '_CancelBatchJobParametersDict', '_CancelBatchJobParametersOrDict', '_ComputeTokensParameters', '_ComputeTokensParametersDict', '_ComputeTokensParametersOrDict', '_CountTokensParameters', '_CountTokensParametersDict', '_CountTokensParametersOrDict', '_CreateBatchJobParameters', '_CreateBatchJobParametersDict', '_CreateBatchJobParametersOrDict', '_CreateCachedContentParameters', '_CreateCachedContentParametersDict', '_CreateCachedContentParametersOrDict', '_CreateFileParameters', '_CreateFileParametersDict', '_CreateFileParametersOrDict', '_CreateTuningJobParameters', '_CreateTuningJobParametersDict', '_CreateTuningJobParametersOrDict', '_DeleteBatchJobParameters', '_DeleteBatchJobParametersDict', '_DeleteBatchJobParametersOrDict', '_DeleteCachedContentParameters', '_DeleteCachedContentParametersDict', '_DeleteCachedContentParametersOrDict', '_DeleteFileParameters', '_DeleteFileParametersDict', '_DeleteFileParametersOrDict', '_DeleteModelParameters', '_DeleteModelParametersDict', '_DeleteModelParametersOrDict', '_EditImageParameters', '_EditImageParametersDict', '_EditImageParametersOrDict', '_EmbedContentParameters', '_EmbedContentParametersDict', '_EmbedContentParametersOrDict', '_FetchPredictOperationParameters', '_FetchPredictOperationParametersDict', '_FetchPredictOperationParametersOrDict', '_GenerateContentParameters', '_GenerateContentParametersDict', '_GenerateContentParametersOrDict', '_GenerateImagesParameters', '_GenerateImagesParametersDict', '_GenerateImagesParametersOrDict', '_GetBatchJobParameters', '_GetBatchJobParametersDict', '_GetBatchJobParametersOrDict', '_GetCachedContentParameters', '_GetCachedContentParametersDict', '_GetCachedContentParametersOrDict', '_GetFileParameters', '_GetFileParametersDict', '_GetFileParametersOrDict', '_GetModelParameters', '_GetModelParametersDict', '_GetModelParametersOrDict', '_GetOperationParameters', '_GetOperationParametersDict', '_GetOperationParametersOrDict', '_GetTuningJobParameters', '_GetTuningJobParametersDict', '_GetTuningJobParametersOrDict', '_ListBatchJobsParameters', '_ListBatchJobsParametersDict', '_ListBatchJobsParametersOrDict', '_ListCachedContentsParameters', '_ListCachedContentsParametersDict', '_ListCachedContentsParametersOrDict', '_ListFilesParameters', '_ListFilesParametersDict', '_ListFilesParametersOrDict', '_ListModelsParameters', '_ListModelsParametersDict', '_ListModelsParametersOrDict', '_ListTuningJobsParameters', '_ListTuningJobsParametersDict', '_ListTuningJobsParametersOrDict', '_ReferenceImageAPI', '_ReferenceImageAPIDict', '_ReferenceImageAPIOrDict', '_UpdateCachedContentParameters', '_UpdateCachedContentParametersDict', '_UpdateCachedContentParametersOrDict', '_UpdateModelParameters', '_UpdateModelParametersDict', '_UpdateModelParametersOrDict', '_UpscaleImageAPIConfig', '_UpscaleImageAPIConfigDict', '_UpscaleImageAPIConfigOrDict', '_UpscaleImageAPIParameters', '_UpscaleImageAPIParametersDict', '_UpscaleImageAPIParametersOrDict', '__annotations__', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '__warningregistry__', '_common', '_is_pillow_image_imported', 'datetime', 'inspect', 'json', 'logging', 'pydantic', 'typing']\n"
     ]
    }
   ],
   "source": [
    "from google import genai\n",
    "from google.genai import Client, types\n",
    "import time\n",
    "\n",
    "print(dir(genai))\n",
    "print(dir(Client))\n",
    "print(dir(types))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0389390d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-12T18:59:14.112149Z",
     "iopub.status.busy": "2025-05-12T18:59:14.111774Z",
     "iopub.status.idle": "2025-05-12T18:59:32.873420Z",
     "shell.execute_reply": "2025-05-12T18:59:32.871962Z",
     "shell.execute_reply.started": "2025-05-12T18:59:14.112122Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded image metadata successfully.\n",
      "Sending request for image_id: 717LpuXhzkL\n",
      "Processed image_id: 717LpuXhzkL\n",
      "Sleeping 60 seconds to respect rate limits...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_31/3944927788.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[0mprogress_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprogress_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf'progress_{current_working_filename}.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m \u001b[0mprocess_records\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlistings_csv_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages_csv_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages_base_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprogress_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_31/3944927788.py\u001b[0m in \u001b[0;36mprocess_records\u001b[0;34m(listings_csv_path, images_csv_path, images_base_path, output_file, progress_file)\u001b[0m\n\u001b[1;32m    142\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mrequests_made\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mMAX_DAILY_REQUESTS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m                     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Sleeping {DELAY_BETWEEN_REQUESTS} seconds to respect rate limits...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m                     \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDELAY_BETWEEN_REQUESTS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0;31m# Run the processing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Constants\n",
    "MAX_DAILY_REQUESTS = 1500\n",
    "DELAY_BETWEEN_REQUESTS = 60\n",
    "\n",
    "# Initialize API client\n",
    "client = genai.Client(api_key=\"\") # Replace with your API key\n",
    "requests_made = 0\n",
    "\n",
    "def load_progress(progress_file):\n",
    "    \"\"\"Load the last processed index from a progress file.\"\"\"\n",
    "    if os.path.exists(progress_file):\n",
    "        with open(progress_file, 'r') as f:\n",
    "            return int(f.read().strip())\n",
    "    return 0\n",
    "\n",
    "def save_progress(progress_file, index):\n",
    "    \"\"\"Save the current processed index to a progress file.\"\"\"\n",
    "    with open(progress_file, 'w') as f:\n",
    "        f.write(str(index))\n",
    "\n",
    "def query_gemini_api(image_bytes, combined_description):\n",
    "    \"\"\"Send image and description to the Gemini API and return generated questions and answers.\"\"\"\n",
    "    prompt_text = (\n",
    "        \"You are given an image and a brief product description.\\n\"\n",
    "        f\"Use the product description context: {combined_description}\\n\"\n",
    "        \"Generate exactly 5 diverse, visually clear, and progressively challenging questions.\\n\"\n",
    "        \"Each question must be answerable by only looking at the image — do NOT rely on external or assumed knowledge.\\n\"\n",
    "        \"Ensure variation in the *type* of visual cues used: color, shape, count, spatial relationship, relative size, and visible text (if any).\\n\"\n",
    "        \"Ensure variation in *difficulty level*:\\n\"\n",
    "        \"- At least 2 simple questions (e.g., color, count)\\n\"\n",
    "        \"- At least 2 moderately difficult questions (e.g., spatial relations, comparisons)\\n\"\n",
    "        \"- 1 challenging question requiring closer inspection or subtle visual reasoning (e.g., most prominent item, inferred use from shape)\\n\"\n",
    "        \"Do NOT ask about materials or properties that are not visually obvious (e.g., plastic, flexible, metal).\\n\"\n",
    "        \"Answers must be a single word — not all of them 'yes' or 'no'.\\n\"\n",
    "        \"Strictly use this format without extra text:\\n\"\n",
    "        \"Question 1: <question>\\n\"\n",
    "        \"Answer 1: <answer>\\n\"\n",
    "        \"Do not include any explanations or extra text.\"\n",
    "    )\n",
    "    try:\n",
    "        response = client.models.generate_content(\n",
    "            model='gemini-2.0-flash',\n",
    "            contents=[\n",
    "                types.Part.from_bytes(data=image_bytes, mime_type='image/jpeg'),\n",
    "                prompt_text\n",
    "            ]\n",
    "        )\n",
    "        return response.text\n",
    "    except Exception as e:\n",
    "        print(f\"Error querying Gemini API: {e}\")\n",
    "        return None\n",
    "\n",
    "def get_image_path_map(images_csv_path):\n",
    "    \"\"\"Create a dictionary mapping image_id to image paths from the CSV.\"\"\"\n",
    "    image_path_map = {}\n",
    "    with open(images_csv_path, 'r', encoding='utf-8') as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for row in reader:\n",
    "            image_path_map[row['image_id']] = row['path']\n",
    "    print(\"Loaded image metadata successfully.\")\n",
    "    return image_path_map\n",
    "\n",
    "def generate_questions_for_image(image_id, row, image_path_map, images_base_path):\n",
    "    \"\"\"Generate questions and answers for a given image.\"\"\"\n",
    "    image_filename = image_path_map.get(image_id)\n",
    "    if not image_filename:\n",
    "        print(f\"Image path not found for image_id: {image_id}\")\n",
    "        return None\n",
    "\n",
    "    full_image_path = os.path.join(images_base_path, image_filename)\n",
    "    if not os.path.exists(full_image_path):\n",
    "        print(f\"Image file does not exist: {full_image_path}\")\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        with open(full_image_path, \"rb\") as img_file:\n",
    "            image_bytes = img_file.read()\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to read image {full_image_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "    combined_description = f\"Overall: {row['overall_description']}; \" \\\n",
    "                           f\"Color: {row['colour_description']}; \" \\\n",
    "                           f\"Material: {row['material_description']}\" \\\n",
    "                           f\"Other: {row['other_description']}; \"\n",
    "    \n",
    "    print(f\"Sending request for image_id: {image_id}\")\n",
    "    return query_gemini_api(image_bytes, combined_description)\n",
    "\n",
    "def write_generated_data(writer, image_id, full_image_path, generated_text):\n",
    "    \"\"\"Write the generated questions and answers to the output CSV file.\"\"\"\n",
    "    lines = [line.strip() for line in generated_text.strip().split('\\n') if line.strip()]\n",
    "    question_lines = [line for line in lines if line.lower().startswith('question')]\n",
    "    answer_lines = [line for line in lines if line.lower().startswith('answer')]\n",
    "\n",
    "    if len(question_lines) == 5 and len(answer_lines) == 5:\n",
    "        for q_line, a_line in zip(question_lines, answer_lines):\n",
    "            question = q_line.split(':', 1)[1].strip()\n",
    "            answer = a_line.split(':', 1)[1].strip()\n",
    "            writer.writerow([image_id, full_image_path, question, answer])\n",
    "\n",
    "def process_records(listings_csv_path, images_csv_path, images_base_path, output_file, progress_file):\n",
    "    \"\"\"Main function to process records, generate questions, and save results.\"\"\"\n",
    "    global requests_made\n",
    "    image_path_map = get_image_path_map(images_csv_path)\n",
    "\n",
    "    output_dir = os.path.dirname(output_file)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    start_index = load_progress(progress_file)\n",
    "    current_index = 0\n",
    "\n",
    "    with open(output_file, 'a', newline='', encoding='utf-8') as f_out:\n",
    "        writer = csv.writer(f_out)\n",
    "        if os.stat(output_file).st_size == 0:\n",
    "            writer.writerow(['image_id', 'full_image_path', 'question', 'answer'])\n",
    "\n",
    "        with open(listings_csv_path, 'r', encoding='utf-8') as f_in:\n",
    "            reader = csv.DictReader(f_in)\n",
    "            for row in reader:\n",
    "                if current_index < start_index:\n",
    "                    current_index += 1\n",
    "                    continue\n",
    "\n",
    "                if requests_made >= MAX_DAILY_REQUESTS:\n",
    "                    print(\"Reached daily request limit. Stopping.\")\n",
    "                    break\n",
    "\n",
    "                image_id = row['main_image_id']\n",
    "                generated_text = generate_questions_for_image(image_id, row, image_path_map, images_base_path)\n",
    "                \n",
    "                if generated_text:\n",
    "                    write_generated_data(writer, image_id, os.path.join(images_base_path, image_path_map.get(image_id)), generated_text)\n",
    "                    print(f\"Processed image_id: {image_id}\")\n",
    "                else:\n",
    "                    print(f\"Failed to generate questions for image_id: {image_id}\")\n",
    "\n",
    "                requests_made += 1\n",
    "                current_index += 1\n",
    "                save_progress(progress_file, current_index)\n",
    "\n",
    "                if requests_made < MAX_DAILY_REQUESTS:\n",
    "                    print(f\"Sleeping {DELAY_BETWEEN_REQUESTS} seconds to respect rate limits...\")\n",
    "                    time.sleep(DELAY_BETWEEN_REQUESTS)\n",
    "\n",
    "# Run the processing\n",
    "current_working_filename = 'listings_3'\n",
    "question_set_number = 'set_4'\n",
    "\n",
    "listings_csv_path = f'/kaggle/working/abo-listings/listings/filtered_metadata/{current_working_filename}.csv'\n",
    "images_csv_path = '/kaggle/input/vrdatasets/abo-images-small/abo-images-small/images/metadata/images.csv'\n",
    "images_base_path = '/kaggle/input/vrdatasets/abo-images-small/abo-images-small/images/small'\n",
    "\n",
    "output_dir = 'generated_questions'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "output_file = os.path.join(output_dir, f'questions_{current_working_filename}_{question_set_number}.csv')\n",
    "\n",
    "progress_dir = 'progress'\n",
    "os.makedirs(progress_dir, exist_ok=True)\n",
    "progress_file = os.path.join(progress_dir, f'progress_{current_working_filename}.txt')\n",
    "\n",
    "process_records(listings_csv_path, images_csv_path, images_base_path, output_file, progress_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a282f000-eff1-4ee0-8095-fec3f138a00d",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 7400339,
     "sourceId": 11786365,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7400596,
     "sourceId": 11786728,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31012,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
