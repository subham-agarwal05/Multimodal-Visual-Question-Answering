{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd087033",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import shutil\n",
    "import os\n",
    "import json\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca5ef10",
   "metadata": {},
   "source": [
    "## Data extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86033502",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_dir = 'abo-listings/listings/metadata/'\n",
    "destination_dir = 'abo-listings/listings/extracted_metadata/'\n",
    "\n",
    "os.makedirs(destination_dir, exist_ok=True)\n",
    "\n",
    "compressed_files = [file for file in os.listdir(source_dir) if file.endswith('.json.gz')]\n",
    "\n",
    "for compressed_file in compressed_files:\n",
    "    src_path = os.path.join(source_dir, compressed_file)\n",
    "    dest_filename = compressed_file.replace('.gz', '')  # strips only .gz\n",
    "    dest_path = os.path.join(destination_dir, dest_filename)\n",
    "\n",
    "    with gzip.open(src_path, 'rb') as source_file, open(dest_path, 'wb') as target_file:\n",
    "        shutil.copyfileobj(source_file, target_file)\n",
    "\n",
    "    print(f\"Decompressed: {compressed_file} → {dest_filename}\")\n",
    "\n",
    "print(\"Extraction process finished.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0eef8ed",
   "metadata": {},
   "source": [
    "## Filtering data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0748c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define directories\n",
    "source_dir = 'abo-listings/listings/extracted_metadata'\n",
    "target_dir = 'abo-listings/listings/filtered_metadata'\n",
    "\n",
    "# Ensure output directory exists\n",
    "os.makedirs(target_dir, exist_ok=True)\n",
    "\n",
    "# Define output CSV header\n",
    "csv_columns = [\n",
    "    'main_image_id', 'overall_description', 'colour_description',\n",
    "    'other_description', 'material_description'\n",
    "]\n",
    "\n",
    "# Utility: Extract 'value' fields filtered by language\n",
    "def extract_values_by_language(entries):\n",
    "    return [\n",
    "        entry['value'] for entry in entries\n",
    "        if 'value' in entry and (\n",
    "            'language_tag' not in entry or entry['language_tag'] in {'en_US', 'en_IN'}\n",
    "        )\n",
    "    ]\n",
    "\n",
    "# Utility: Extract 'standardized_values' from color fields filtered by language\n",
    "def extract_standardized_colors(entries):\n",
    "    standardized = []\n",
    "    for entry in entries:\n",
    "        if 'language_tag' not in entry or entry['language_tag'] in {'en_US', 'en_IN'}:\n",
    "            standardized.extend(entry.get('standardized_values', []))\n",
    "    return standardized\n",
    "\n",
    "# Loop through each JSON file\n",
    "for file_name in os.listdir(source_dir):\n",
    "    if not file_name.endswith('.json'):\n",
    "        continue\n",
    "\n",
    "    json_path = os.path.join(source_dir, file_name)\n",
    "    csv_path = os.path.join(target_dir, file_name.replace('.json', '.csv'))\n",
    "\n",
    "    print(f\"Processing file: {json_path} → {csv_path}\")\n",
    "\n",
    "    # Load JSON lines\n",
    "    records = []\n",
    "    with open(json_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            line = line.strip()\n",
    "            if line:\n",
    "                try:\n",
    "                    records.append(json.loads(line))\n",
    "                except json.JSONDecodeError as error:\n",
    "                    print(f\"Error parsing line in {file_name}: {error}\")\n",
    "\n",
    "    # Filter out incomplete or irrelevant records\n",
    "    required_fields = {\n",
    "        'brand', 'bullet_point', 'color', 'model_name',\n",
    "        'item_name', 'product_type', 'main_image_id',\n",
    "        'item_keywords', 'country'\n",
    "    }\n",
    "\n",
    "    valid_records = [\n",
    "        rec for rec in records\n",
    "        if required_fields.issubset(rec.keys()) and rec.get('country') in {'IN', 'US'}\n",
    "    ]\n",
    "\n",
    "    print(f\" → Valid entries found: {len(valid_records)}\")\n",
    "\n",
    "    # Write filtered records to CSV\n",
    "    with open(csv_path, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow(csv_columns)\n",
    "\n",
    "        for rec in valid_records:\n",
    "            overall_desc = extract_values_by_language(rec.get('bullet_point', []))\n",
    "\n",
    "            color_desc = []\n",
    "            color_desc += extract_standardized_colors(rec.get('color', []))\n",
    "            color_desc += extract_values_by_language(rec.get('color', []))\n",
    "\n",
    "            other_desc = []\n",
    "            for key in ['product_type', 'item_keywords']:\n",
    "                other_desc += extract_values_by_language(rec.get(key, []))\n",
    "\n",
    "            material_desc = extract_values_by_language(rec.get('material', [])) if 'material' in rec else []\n",
    "\n",
    "            row = [\n",
    "                rec.get('main_image_id', ''),\n",
    "                '; '.join(overall_desc),\n",
    "                '; '.join(color_desc),\n",
    "                '; '.join(other_desc),\n",
    "                '; '.join(material_desc)\n",
    "            ]\n",
    "            writer.writerow(row)\n",
    "\n",
    "    print(f\" → Output written: {csv_path} ({len(valid_records)} rows)\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd20b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade pip\n",
    "!pip install google-genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83fa80a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai\n",
    "from google.genai import Client, types\n",
    "import time\n",
    "\n",
    "print(dir(genai))\n",
    "print(dir(Client))\n",
    "print(dir(types))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0389390d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import json\n",
    "import time\n",
    "from google.generativeai import Client, types\n",
    "\n",
    "# ==== Configuration ====\n",
    "API_KEY = \"\"  # Insert your Gemini API key here\n",
    "DAILY_LIMIT = 1500\n",
    "REQUEST_INTERVAL = 60  # seconds\n",
    "\n",
    "# ==== Client Setup ====\n",
    "gemini = Client(api_key=API_KEY)\n",
    "\n",
    "# ==== Progress Tracker ====\n",
    "def read_progress(file_path):\n",
    "    if os.path.isfile(file_path):\n",
    "        with open(file_path, 'r') as f:\n",
    "            return int(f.read().strip())\n",
    "    return 0\n",
    "\n",
    "def write_progress(file_path, index):\n",
    "    with open(file_path, 'w') as f:\n",
    "        f.write(str(index))\n",
    "\n",
    "# ==== API Interaction ====\n",
    "def generate_visual_questions(image_bytes, description_text):\n",
    "    prompt = (\n",
    "        \"You will receive an image along with a short product description.\\n\"\n",
    "        f\"Refer to this product description for context: {description_text}\\n\"\n",
    "        \"Create exactly 5 visually-based questions that increase in difficulty and are varied in nature.\\n\"\n",
    "        \"Each question must be answerable *solely* through visual inspection of the image — do not use external knowledge or assumptions.\\n\"\n",
    "        \"Incorporate a mix of visual features across questions, such as: color, number of elements, shapes, positioning, relative size, and any visible text.\\n\"\n",
    "        \"Ensure a balance in difficulty:\\n\"\n",
    "        \"- 2 questions should be easy (e.g., identify a color or count elements)\\n\"\n",
    "        \"- 2 should be of medium complexity (e.g., spatial arrangement, size comparisons)\\n\"\n",
    "        \"- 1 should be more difficult, requiring close observation or visual reasoning (e.g., identifying a main feature or deducing purpose from form)\\n\"\n",
    "        \"Avoid asking about non-visible attributes like materials or internal functions.\\n\"\n",
    "        \"Each answer must be a *single word* and answers should not all be 'yes' or 'no'.\\n\"\n",
    "        \"Format your output exactly like this — do not include any extra comments or explanations:\\n\"\n",
    "        \"Question 1: <your question>\\n\"\n",
    "        \"Answer 1: <your one-word answer>\"\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        response = gemini.models.generate_content(\n",
    "            model='gemini-2.0-flash',\n",
    "            contents=[\n",
    "                types.Part.from_bytes(data=image_bytes, mime_type='image/jpeg'),\n",
    "                prompt\n",
    "            ]\n",
    "        )\n",
    "        return response.text\n",
    "    except Exception as e:\n",
    "        print(f\"API error: {e}\")\n",
    "        return None\n",
    "\n",
    "# ==== Main Processing Logic ====\n",
    "def generate_questions_for_dataset(metadata_csv, images_csv, image_root, output_csv, progress_txt):\n",
    "    image_map = {}\n",
    "    request_count = 0\n",
    "\n",
    "    # Load image paths\n",
    "    with open(images_csv, 'r', encoding='utf-8') as img_file:\n",
    "        reader = csv.DictReader(img_file)\n",
    "        for row in reader:\n",
    "            image_map[row['image_id']] = row['path']\n",
    "    print(\"Image metadata loaded.\")\n",
    "\n",
    "    os.makedirs(os.path.dirname(output_csv), exist_ok=True)\n",
    "    resume_from = read_progress(progress_txt)\n",
    "    current_line = 0\n",
    "\n",
    "    with open(output_csv, 'a', newline='', encoding='utf-8') as out_file:\n",
    "        writer = csv.writer(out_file)\n",
    "        if os.stat(output_csv).st_size == 0:\n",
    "            writer.writerow(['image_id', 'image_path', 'question', 'answer'])\n",
    "\n",
    "        with open(metadata_csv, 'r', encoding='utf-8') as data_file:\n",
    "            reader = csv.DictReader(data_file)\n",
    "            for entry in reader:\n",
    "                if current_line < resume_from:\n",
    "                    current_line += 1\n",
    "                    continue\n",
    "\n",
    "                if request_count >= DAILY_LIMIT:\n",
    "                    print(\"Daily request cap reached.\")\n",
    "                    break\n",
    "\n",
    "                img_id = entry['main_image_id']\n",
    "                img_rel_path = image_map.get(img_id)\n",
    "                if not img_rel_path:\n",
    "                    print(f\"No image path found for: {img_id}\")\n",
    "                    current_line += 1\n",
    "                    continue\n",
    "\n",
    "                full_path = os.path.join(image_root, img_rel_path)\n",
    "                if not os.path.exists(full_path):\n",
    "                    print(f\"Missing image file: {full_path}\")\n",
    "                    current_line += 1\n",
    "                    continue\n",
    "\n",
    "                try:\n",
    "                    with open(full_path, 'rb') as img:\n",
    "                        img_data = img.read()\n",
    "                except Exception as e:\n",
    "                    print(f\"Failed to read image {full_path}: {e}\")\n",
    "                    current_line += 1\n",
    "                    continue\n",
    "\n",
    "                description = (\n",
    "                    f\"Overall: {entry['overall_description']}; \"\n",
    "                    f\"Color: {entry['colour_description']}; \"\n",
    "                    f\"Material: {entry['material_description']}; \"\n",
    "                    f\"Other: {entry['other_description']}\"\n",
    "                )\n",
    "\n",
    "                print(f\"Requesting for {img_id}...\")\n",
    "                response_text = generate_visual_questions(img_data, description)\n",
    "\n",
    "                if response_text:\n",
    "                    lines = [line.strip() for line in response_text.split('\\n') if line.strip()]\n",
    "                    questions = [l for l in lines if l.lower().startswith('question')]\n",
    "                    answers = [l for l in lines if l.lower().startswith('answer')]\n",
    "\n",
    "                    if len(questions) == 5 and len(answers) == 5:\n",
    "                        for q, a in zip(questions, answers):\n",
    "                            question = q.split(':', 1)[1].strip()\n",
    "                            answer = a.split(':', 1)[1].strip()\n",
    "                            writer.writerow([img_id, full_path, question, answer])\n",
    "                            out_file.flush()\n",
    "                        print(f\"Finished: {img_id}\")\n",
    "                    else:\n",
    "                        print(f\"Incomplete or misformatted response for: {img_id}\")\n",
    "                else:\n",
    "                    print(f\"Generation failed for: {img_id}\")\n",
    "\n",
    "                request_count += 1\n",
    "                current_line += 1\n",
    "                write_progress(progress_txt, current_line)\n",
    "\n",
    "                if request_count < DAILY_LIMIT:\n",
    "                    print(f\"Waiting {REQUEST_INTERVAL} seconds...\")\n",
    "                    time.sleep(REQUEST_INTERVAL)\n",
    "\n",
    "# ==== Execution Parameters ====\n",
    "filename_tag = 'listings_3'\n",
    "question_batch = 'set_4'\n",
    "\n",
    "csv_metadata = f'abo-listings/listings/filtered_metadata/{filename_tag}.csv'\n",
    "csv_images = 'abo-images-small/images/metadata/images.csv'\n",
    "image_directory = 'abo-images-small/images/small'\n",
    "\n",
    "output_dir = 'generated_questions'\n",
    "progress_dir = 'progress'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "os.makedirs(progress_dir, exist_ok=True)\n",
    "\n",
    "csv_output = os.path.join(output_dir, f'questions_{filename_tag}_{question_batch}.csv')\n",
    "progress_marker = os.path.join(progress_dir, f'progress_{filename_tag}.txt')\n",
    "\n",
    "# ==== Start Processing ====\n",
    "generate_questions_for_dataset(csv_metadata, csv_images, image_directory, csv_output, progress_marker)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
