{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd087033",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import shutil\n",
    "import os\n",
    "import json\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cceb208f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86033502",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = 'abo-listings/listings/metadata/'\n",
    "output_dir = 'abo-listings/listings/extracted_metadata/'\n",
    "\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "for filename in os.listdir(input_dir):\n",
    "    if filename.endswith('.json.gz'):\n",
    "        input_path = os.path.join(input_dir, filename)\n",
    "        output_filename = filename[:-3]  # remove the .gz extension\n",
    "        output_path = os.path.join(output_dir, output_filename)\n",
    "        \n",
    "        with gzip.open(input_path, 'rb') as f_in:\n",
    "            with open(output_path, 'wb') as f_out:\n",
    "                shutil.copyfileobj(f_in, f_out)\n",
    "        \n",
    "        print(f\"Extracted {filename} → {output_filename}\")\n",
    "\n",
    "print(\"All extractions complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d95fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_dir = 'abo-listings/listings/extracted_metadata/'\n",
    "\n",
    "for filename in os.listdir(json_dir):\n",
    "    if filename.endswith('.json'):\n",
    "        file_path = os.path.join(json_dir, filename)\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                content = f.read().strip()\n",
    "                \n",
    "                if content.startswith('['):\n",
    "                    data = json.loads(content)\n",
    "                    print(f\"{filename} loaded as JSON array. Length: {len(data)}\")\n",
    "                else:\n",
    "                    items = []\n",
    "                    for line in content.splitlines():\n",
    "                        line = line.strip()\n",
    "                        if line:\n",
    "                            items.append(json.loads(line))\n",
    "                    print(f\"{filename} loaded as {len(items)} separate JSON objects (line by line).\")\n",
    "                    \n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"{filename} failed to load: {e}\")\n",
    "        except Exception as e:\n",
    "            print(f\"{filename} encountered an error: {e}\")\n",
    "\n",
    "print(\"JSON structure check complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af394b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'abo-listings/listings/extracted_metadata/listings_3.json'\n",
    "\n",
    "print(f\"Inspecting {file_path}\")\n",
    "\n",
    "# Read the raw content\n",
    "with open(file_path, 'r', encoding='utf-8') as f:\n",
    "    raw = f.read()\n",
    "\n",
    "# Show first 500 characters for manual inspection\n",
    "print(\"\\n--- First 500 characters ---\")\n",
    "print(raw[:500])\n",
    "print(\"\\n---------------------------\")\n",
    "\n",
    "# Try loading as a single JSON object\n",
    "try:\n",
    "    data = json.loads(raw)\n",
    "    print(\" Loaded as single JSON object\")\n",
    "    print(f\"Type: {type(data)}\")\n",
    "    if isinstance(data, dict):\n",
    "        print(f\"Top-level keys: {list(data.keys())}\")\n",
    "    elif isinstance(data, list):\n",
    "        print(f\"List length: {len(data)}\")\n",
    "        print(f\"First item type: {type(data[0])}\")\n",
    "        if isinstance(data[0], dict):\n",
    "            print(f\"First item keys: {list(data[0].keys())}\")\n",
    "except json.JSONDecodeError as e:\n",
    "    print(f\"Failed to load as single JSON: {e}\")\n",
    "\n",
    "    # Try line by line\n",
    "    print(\"\\nTrying to parse line by line...\")\n",
    "    items = []\n",
    "    for line in raw.splitlines():\n",
    "        line = line.strip()\n",
    "        if line:\n",
    "            try:\n",
    "                obj = json.loads(line)\n",
    "                items.append(obj)\n",
    "            except Exception as sub_e:\n",
    "                print(f\"Failed to parse line: {sub_e}\")\n",
    "                break\n",
    "\n",
    "    print(f\"Parsed {len(items)} JSON objects (line by line)\")\n",
    "    if items:\n",
    "        print(f\"First item type: {type(items[0])}\")\n",
    "        if isinstance(items[0], dict):\n",
    "            print(f\"First item keys: {list(items[0].keys())}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0748c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = 'abo-listings/listings/extracted_metadata'\n",
    "output_dir = 'abo-listings/listings/filtered_metadata'\n",
    "\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "header = [\n",
    "    'main_image_id', 'overall_description', 'colour_description', 'other_description', 'material_description'\n",
    "]\n",
    "\n",
    "# Function to filter 'value' fields by language_tag (or accept if no language_tag)\n",
    "def get_filtered_values(entries):\n",
    "    filtered_values = []\n",
    "    for entry in entries:\n",
    "        value = entry.get('value')\n",
    "        language_tag = entry.get('language_tag')\n",
    "        if value and (language_tag is None or language_tag in ['en_IN', 'en_US']):\n",
    "            filtered_values.append(value)\n",
    "    return filtered_values\n",
    "\n",
    "# Function to filter 'standardized_values' by language_tag (or accept if no language_tag)\n",
    "def get_filtered_standardized_values(color_entries):\n",
    "    filtered_values = []\n",
    "    for entry in color_entries:\n",
    "        language_tag = entry.get('language_tag')\n",
    "        if language_tag is None or language_tag in ['en_IN', 'en_US']:\n",
    "            std_values = entry.get('standardized_values', [])\n",
    "            filtered_values.extend(std_values)\n",
    "    return filtered_values\n",
    "\n",
    "# Process each JSON file in the input directory\n",
    "for filename in os.listdir(input_dir):\n",
    "    if filename.endswith('.json'):\n",
    "        input_file = os.path.join(input_dir, filename)\n",
    "        output_file = os.path.join(output_dir, filename.replace('.json', '.csv'))\n",
    "\n",
    "        print(f\"Processing {input_file} → {output_file}\")\n",
    "\n",
    "        # Load line-delimited JSON\n",
    "        records = []\n",
    "        with open(input_file, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                try:\n",
    "                    records.append(json.loads(line))\n",
    "                except json.JSONDecodeError as e:\n",
    "                    print(f\"Failed to load a line in {filename}: {e}\")\n",
    "\n",
    "        required_keys = ['brand', 'bullet_point', 'color', 'model_name', 'item_name', \n",
    "                         'product_type', 'main_image_id', 'item_keywords', 'country']\n",
    "\n",
    "        filtered_records = [\n",
    "            record for record in records\n",
    "            if all(key in record for key in required_keys)\n",
    "            and record.get('country') in ['IN', 'US']\n",
    "            # and 'item_dimensions' not in record\n",
    "        ]\n",
    "\n",
    "        print(f\" → Total matching records: {len(filtered_records)}\")\n",
    "\n",
    "        with open(output_file, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "            writer = csv.writer(csvfile)\n",
    "            writer.writerow(header) \n",
    "\n",
    "            for record in filtered_records:\n",
    "                overall_description = get_filtered_values(record.get('bullet_point', []))\n",
    "                colour_description = []\n",
    "                colour_description.extend(get_filtered_standardized_values(record.get('color', [])))\n",
    "                colour_description.extend(get_filtered_values(record.get('color', [])))\n",
    "                other_description = []\n",
    "                for field in ['product_type', 'item_keywords']:\n",
    "                    other_description.extend(get_filtered_values(record.get(field, [])))\n",
    "                material_description = []\n",
    "                if 'material' in record:\n",
    "                    material_description.extend(get_filtered_values(record.get('material', [])))\n",
    "\n",
    "                row = [\n",
    "                    record.get('main_image_id'),\n",
    "                    '; '.join(overall_description),\n",
    "                    '; '.join(colour_description),\n",
    "                    '; '.join(other_description),\n",
    "                    '; '.join(material_description)\n",
    "                ]\n",
    "\n",
    "                writer.writerow(row)\n",
    "\n",
    "        print(f\" → Saved {len(filtered_records)} records to {output_file}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd20b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade pip\n",
    "!pip install google-genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83fa80a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai\n",
    "from google.genai import Client, types\n",
    "import time\n",
    "\n",
    "print(dir(genai))\n",
    "print(dir(Client))\n",
    "print(dir(types))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0389390d",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = genai.Client(api_key=\"\") #insert api key here \n",
    "\n",
    "# Set daily limits\n",
    "MAX_DAILY_REQUESTS = 1500\n",
    "DELAY_BETWEEN_REQUESTS = 60 \n",
    "\n",
    "requests_made = 0\n",
    "\n",
    "def load_progress(progress_file):\n",
    "    if os.path.exists(progress_file):\n",
    "        with open(progress_file, 'r') as f:\n",
    "            index = f.read().strip()\n",
    "            return int(index)\n",
    "    return 0\n",
    "\n",
    "def save_progress(progress_file, index):\n",
    "    with open(progress_file, 'w') as f:\n",
    "        f.write(str(index))\n",
    "\n",
    "def query_gemini_api(image_bytes, combined_description):\n",
    "    prompt_text = (\n",
    "        \"You will receive an image along with a short product description.\\n\"\n",
    "        f\"Refer to this product description for context: {combined_description}\\n\"\n",
    "        \"Create exactly 5 visually-based questions that increase in difficulty and are varied in nature.\\n\"\n",
    "        \"Each question must be answerable *solely* through visual inspection of the image — do not use external knowledge or assumptions.\\n\"\n",
    "        \"Incorporate a mix of visual features across questions, such as: color, number of elements, shapes, positioning, relative size, and any visible text.\\n\"\n",
    "        \"Ensure a balance in difficulty:\\n\"\n",
    "        \"- 2 questions should be easy (e.g., identify a color or count elements)\\n\"\n",
    "        \"- 2 should be of medium complexity (e.g., spatial arrangement, size comparisons)\\n\"\n",
    "        \"- 1 should be more difficult, requiring close observation or visual reasoning (e.g., identifying a main feature or deducing purpose from form)\\n\"\n",
    "        \"Avoid asking about non-visible attributes like materials or internal functions.\\n\"\n",
    "        \"Each answer must be a *single word* and answers should not all be 'yes' or 'no'.\\n\"\n",
    "        \"Format your output exactly like this — do not include any extra comments or explanations:\\n\"\n",
    "        \"Question 1: <your question>\\n\"\n",
    "        \"Answer 1: <your one-word answer>\"\n",
    "    )\n",
    "\n",
    "\n",
    "    try:\n",
    "        response = client.models.generate_content(\n",
    "            model='gemini-2.0-flash',\n",
    "            contents=[\n",
    "                types.Part.from_bytes(\n",
    "                    data=image_bytes,\n",
    "                    mime_type='image/jpeg'\n",
    "                ),\n",
    "                prompt_text\n",
    "            ]\n",
    "        )\n",
    "        return response.text\n",
    "    except Exception as e:\n",
    "        print(f\"Error querying Gemini API: {e}\")\n",
    "        return None\n",
    "\n",
    "def process_records(listings_csv_path, images_csv_path, images_base_path, output_file, progress_file):\n",
    "    global requests_made\n",
    "\n",
    "    image_path_map = {}\n",
    "    with open(images_csv_path, 'r', encoding='utf-8') as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for row in reader:\n",
    "            image_path_map[row['image_id']] = row['path']\n",
    "\n",
    "    print(\"Loaded image metadata successfully.\")\n",
    "\n",
    "    output_dir = os.path.dirname(output_file)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Load last processed index\n",
    "    start_index = load_progress(progress_file)\n",
    "    current_index = 0\n",
    "\n",
    "    with open(output_file, 'a', newline='', encoding='utf-8') as f_out:\n",
    "        writer = csv.writer(f_out)\n",
    "        if os.stat(output_file).st_size == 0:\n",
    "            writer.writerow(['image_id', 'full_image_path', 'question', 'answer'])\n",
    "\n",
    "        with open(listings_csv_path, 'r', encoding='utf-8') as f_in:\n",
    "            reader = csv.DictReader(f_in)\n",
    "            for row in reader:\n",
    "                if current_index < start_index:\n",
    "                    current_index += 1\n",
    "                    continue  # skip already processed\n",
    "\n",
    "                if requests_made >= MAX_DAILY_REQUESTS:\n",
    "                    print(\"Reached daily request limit. Stopping.\")\n",
    "                    break\n",
    "\n",
    "                image_id = row['main_image_id']\n",
    "                image_filename = image_path_map.get(image_id)\n",
    "\n",
    "                if not image_filename:\n",
    "                    print(f\"Image path not found for image_id: {image_id}\")\n",
    "                    current_index += 1\n",
    "                    continue\n",
    "\n",
    "                full_image_path = os.path.join(images_base_path, image_filename)\n",
    "\n",
    "                if not os.path.exists(full_image_path):\n",
    "                    print(f\"Image file does not exist: {full_image_path}\")\n",
    "                    current_index += 1\n",
    "                    continue\n",
    "\n",
    "                try:\n",
    "                    with open(full_image_path, \"rb\") as img_file:\n",
    "                        image_bytes = img_file.read()\n",
    "                except Exception as e:\n",
    "                    print(f\"Failed to read image {full_image_path}: {e}\")\n",
    "                    current_index += 1\n",
    "                    continue\n",
    "\n",
    "                combined_description = f\"Overall: {row['overall_description']}; \" \\\n",
    "                                       f\"Color: {row['colour_description']}; \" \\\n",
    "                                       f\"Material: {row['material_description']}\"\\\n",
    "                                       f\"Other: {row['other_description']}; \" \\\n",
    "\n",
    "                print(f\"Sending request for image_id: {image_id}\")\n",
    "\n",
    "                generated_text = query_gemini_api(image_bytes, combined_description)\n",
    "\n",
    "                if generated_text:\n",
    "                    lines = [line.strip() for line in generated_text.strip().split('\\n') if line.strip()]\n",
    "                    question_lines = [line for line in lines if line.lower().startswith('question')]\n",
    "                    answer_lines = [line for line in lines if line.lower().startswith('answer')]\n",
    "\n",
    "                    if len(question_lines) == 5 and len(answer_lines) == 5:\n",
    "                        for q_line, a_line in zip(question_lines, answer_lines):\n",
    "                            question = q_line.split(':', 1)[1].strip()\n",
    "                            answer = a_line.split(':', 1)[1].strip()\n",
    "                            writer.writerow([image_id, full_image_path, question, answer])\n",
    "                            f_out.flush()\n",
    "                        print(f\"Processed image_id: {image_id}\")\n",
    "                    else:\n",
    "                        print(f\"Unexpected format or count in response for image_id: {image_id}\")\n",
    "                else:\n",
    "                    print(f\"Failed to generate questions for image_id: {image_id}\")\n",
    "\n",
    "                requests_made += 1\n",
    "                current_index += 1\n",
    "                save_progress(progress_file, current_index)\n",
    "\n",
    "                if requests_made < MAX_DAILY_REQUESTS:\n",
    "                    print(f\"Sleeping {DELAY_BETWEEN_REQUESTS} seconds to respect rate limits...\")\n",
    "                    time.sleep(DELAY_BETWEEN_REQUESTS)\n",
    "\n",
    "import os\n",
    "\n",
    "current_working_filename = 'listings_3'\n",
    "question_set_number = 'set_4'\n",
    "\n",
    "\n",
    "listings_csv_path = f'abo-listings/listings/filtered_metadata/{current_working_filename}.csv'\n",
    "images_csv_path = 'abo-images-small/images/metadata/images.csv'\n",
    "images_base_path = 'abo-images-small/images/small'\n",
    "\n",
    "output_dir = 'generated_questions'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "output_file = os.path.join(output_dir, f'questions_{current_working_filename}_{question_set_number}.csv')\n",
    "\n",
    "progress_dir = 'progress'\n",
    "os.makedirs(progress_dir, exist_ok=True)\n",
    "progress_file = os.path.join(progress_dir, f'progress_{current_working_filename}.txt')\n",
    "\n",
    "\n",
    "process_records(listings_csv_path, images_csv_path, images_base_path, output_file, progress_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
